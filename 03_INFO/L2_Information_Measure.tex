\documentclass[9pt]{beamer}
\usepackage{kotex}
\usepackage{amsfonts,amssymb,amsthm}
\usepackage[dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage{etoolbox}
\usepackage{braket}
%## color
\definecolor{customBlack}{HTML}{3B4252}
\definecolor{customBlackGrey}{HTML}{434C5e}
\definecolor{cuatomGrey}{HTML}{4C566A} 
\definecolor{customWhite}{HTML}{ECEFF4} 
\definecolor{customBlue}{HTML}{6082B6}  
\definecolor{customRed}{HTML}{BF616A}
\definecolor{vividauburn}{rgb}{0.58, 0.15, 0.14}


%## Theme & custom
% \usetheme{metropolis}           % Use metropolis theme
% \metroset{block=fill}
\usetheme{moloch} % modern fork of the metropolis theme
\molochset{block=fill}
\setbeamersize{text margin left=5mm, text margin right=5mm}
\setbeamercolor{palette primary}{bg=customBlack}
\setbeamercolor{alerted text}{fg=customRed}
\setbeamercolor{itemize item}{fg=customBlue}


%## font
\usefonttheme[onlymath]{serif}
% \setbeamerfont{normal text}{size=\small}
% \setbeamerfont{math text}{size=\tiny}


%## Theorem title, numbering
\makeatletter
\setbeamertemplate{theorem begin}
{%
\begin{\inserttheoremblockenv}
{%
\inserttheoremheadfont
\inserttheoremname
\ifx\inserttheoremaddition\@empty\else\ of\ \inserttheoremaddition\fi%
\inserttheorempunctuation
}%
}
\setbeamertemplate{theorem end}{\end{\inserttheoremblockenv}}
\makeatother
\setbeamertemplate{theorems}[numbered]  


%## Custom block
\setbeamercolor{block title}{bg=customBlue, fg=white}
\setbeamercolor{block body}{bg=customWhite, fg=customBlack}
\setbeamercolor{block title alerted}{%
    use={block title, alerted text},
    bg=customRed,
    fg=white
}
\setbeamercolor{block body alerted}{%
    use={block title, alerted text},
    bg=customWhite,
    fg=customBlack
}
\AtBeginEnvironment{definition}{%
    \setbeamercolor{block title}{fg=white,bg=customBlackGrey}
    \setbeamercolor{block body}{fg=customBlack, bg=customWhite}
}
\AtBeginEnvironment{theorem}{%
    \setbeamercolor{block title}{fg=white,bg=customBlackGrey}
    \setbeamercolor{block body}{fg=customBlack, bg=customWhite}
}
\AtBeginEnvironment{corollary}{%
    \setbeamercolor{block title}{fg=white,bg=customBlackGrey}
    \setbeamercolor{block body}{fg=customBlack, bg=customWhite}
}
\AtBeginEnvironment{lemma}{%
    \setbeamercolor{block title}{fg=white,bg=customBlackGrey}
    \setbeamercolor{block body}{fg=customBlack, bg=customWhite}
}


%! Useful command
\renewcommand{\Pr}{\text{Pr}}
% $\ast$ \underline{Proof}:
%\checkmark \underline{meaning}:

\title{2. Information Measure}
\date{\today}
\author{Vaughan Sohn}
% \institute{Centre for Modern Beamer Themes}


\begin{document}
    %#################################### 
    \maketitle
    
    %#################################### 
    \begin{frame}
        \frametitle{Contents}
        \tableofcontents
    \end{frame}

    %#################################### 
    \begin{section}{Entropy}
        \begin{frame}{Entropy}
            % "Information" 이라는 추상적인 단위를 어떻게 수학적으로 정의할 수 있을까?
            \begin{block}{Represent INFORMATION}
                어떤 사건 $E$가 발생했을 때, 그 사건이 매우 \textit{희귀하다면} 우리에게 많은 정보를 제공해주겠지만, 매우 흔한 사건이라면 별 다른 정보를 제공해주지 않을 것이다.
                \\ $\Rightarrow$ 이러한 직관에 기반하여, entropy를 다음과 같이 정의해보자.
            \end{block}
            \begin{definition}[entropy on event]
                For the event $E$, we define a measure of information, \textbf{entropy} $H(E) \in \mathbb R^+$ that satisfies the following properties:
                \begin{itemize}
                    \item Function of $P(E)$
                    \item Continuous in $P(E)$
                    \item If $P(E)$ is increasing, then entropy $H(E)$ is decreasing
                    \item If $E_1 \perp E_2$, then joint entropy is just addition of each entropy
                    $$ H(E_1 \cap E_2) = H(E_1) + H(E_2)$$
                \end{itemize}
                Therefore, the entropy can be defined by the following function,
                $$H(E) \triangleq -\log P(E) \text{(bits)}.$$
            \end{definition}
        \end{frame}

        \begin{frame}{Entropy}
            일반화하면, 특정한 event $E$가 아니라 random variable; experiment에 대한 entropy도 다음과 같이 정의할 수 있다.
            \\ $\Rightarrow$ \alert{Average} amount of information by observing the realization of $X$. (i.e., $x \in \mathcal X$)
            \begin{definition}[entropy on random variable]
                The entropy $H(X)$ of a discrete random variable $X$ is defined by
                $$H(X) = - \sum_{x \in \mathcal X} P_X(x) \log P_X(x)$$
            \end{definition}
            \vspace{0.3cm}
            By definition, entropy has following properties:
            \begin{itemize}
                \item $H(P)\ge 0$, with equality iff $P$ is deterministic.
                \item $H(P)$ is continuous in $P \in \mathbb R^{|\mathcal X|}$
                \item $H$ is \textit{divisible} with successive choices
                \\ Example:
                $$ H([1 / 2,1 / 3,1 / 6])=H([1 / 2,1 / 2])+\frac{1}{2} H([2 / 3,1 / 3]) $$
            \end{itemize}
            \vspace{0.5cm}
        \end{frame}


        \begin{frame}{Examples}
            Examples:
             \begin{itemize}
                \item Binary random variable
                $X \sim \text{Bernoulli}(p)$인 r.v.에 대해 entropy를 구하라.
                $$H_B(p) \triangleq \qquad \qquad \qquad \qquad \qquad \qquad  \qquad \qquad \qquad \qquad \qquad \qquad  \qquad \qquad $$
                \vspace{2cm}
                \item Random variable uniformly distributed over a finite set
                r.v. $U$에 대한 sample space가 $\mathcal U = \{1, 2, \cdots, M\}$이고 uniform distribution을 따를때, entropy를 구하라.
                $$H(U) \triangleq \qquad \qquad \qquad \qquad \qquad \qquad  \qquad \qquad \qquad \qquad \qquad \qquad  \qquad \qquad $$
            \end{itemize}
        \end{frame}

        \begin{frame}{Multivariable entropy}
            \begin{definition}[joint entropy]
                The \textbf{joint entropy} $H(X, Y)$ of a pair of discrete random variables $(X, Y)$ with a joint distribution $P_{X, Y}(x,y)$ is defined as 
                $$ H(X, Y) = -\sum_{x \in \mathcal X} \sum_{y \in \mathcal Y} P_{X, Y}(x,y) \log P_{X,Y}(x,y)$$
            \end{definition}            
            \begin{definition}[conditional entropy on observable]
                The \textbf{conditional entropy} of $Y$, conditioned on $X=x$ is defined as
                $$ H(Y | X=x) = -\sum_{y \in \mathcal{Y}} {P_{Y | X}(y | x)} \log P_{Y| X}(y | x). $$
            \end{definition}
        \end{frame}

        \begin{frame}{Multivariable Entropy}
            \begin{definition}[conditional entropy on r.v.]
                The \textbf{conditional entropy} of $Y$, conditioned on $X$ is defined as
                $$ H(Y | X) = \mathbb E_{P_X}[H(Y|X=x)] = \mathbb E_{P_{XY}}[-\log P_{Y|X} (Y|X)] $$
            \end{definition}
            \vspace{0.2cm}
            \checkmark \underline {meaning:} Random variable $X$에 대한 entropy $H(X)$가 가능한 outcome $x \in \mathcal X$의 entropy $H(X=x)$의 \alert{expectation}으로 정의되는 것처럼, random variable $X$자체에 대한 conditioned entropy $H(Y|X)$는 $X$가 가질 수 있는 모든 outcome $x \in \mathcal X$에 대한 \alert{conditioned entropy$H(Y|X=x)$의 expectation}으로 정의할 수 있다. \\
            \vspace{0.4cm}
            $\ast$ \underline{Proof}:
            Joint probability에 대한 표현으로 전환하는 과정을 기술하면 다음과 같다.
            \\$\Rightarrow$
            \vspace{2cm}
            
        \end{frame}

        \begin{frame}{Multivariable Entropy}
            \begin{theorem}[chain rule]
                \vspace{-0.4cm}
                \begin{align*} H(X,Y)& =H(X)+H(Y | X)  \\ &=H(Y) + H(X|Y) \end{align*}
            \end{theorem}
            \vspace{0.4cm}
            $\ast$ \underline{Proof}:
            \\$\Rightarrow$
            \vspace{3cm}
        \end{frame}

        \begin{frame}{Examples}
            Example: 다음의 joint probability가 주어졌을 때, 각각의 entropy를 구하라.
            $$ \begin{array}{c|cc}  & \mathrm{X}=0 & \mathrm{X}=1 \\ \hline \mathrm{Y}=0 & 1 / 2 & 1 / 3 \\ \mathrm{Y}=1 & 0 & 1 / 6 \end{array}$$
            \begin{itemize}
                \item $H(X), H(Y)$
                \item $H(X,Y)$
                \item $H(Y|x=0),\ H(Y|x=1)$
                \item $H(Y|X)$
            \end{itemize}
            $\Rightarrow$
            \vspace{3cm}
        \end{frame}
    \end{section}

    %#################################### 
    \begin{section}{Mutual Information}
        % 패널 그림 하나 넣기
        \begin{frame}{Mutual Information}

            \begin{definition}[mutual information]
                The \textbf{mutual information} $I(X;Y)$ is defined as
                \begin{align*}
                    I(X;Y) \triangleq H(X) - H(X|Y)
                \end{align*}
            \end{definition}
            \checkmark \underline {meaning:} $H(X)$가 $X$에 대한 정보[$\ast$], $H(X|Y)$가 $Y$를 알았을 때 $X$에 대해 남아있는 정보이므로 $I(X;Y)$는 \alert{$Y$를 알게됨으로서 얻은 $X$에 대한 정보}로 해석할 수 있다.
            \vspace{0.3cm}
            \begin{itemize}
                \item Dependency on channel $W=P_{Y|X}$,
                \begin{itemize}
                    \item 채널이 완전하다면 $I(X; Y) = H(X) \leftrightarrow H(X|Y) =0$
                    \item 채널이 불완전하여 손실되는 정보가 있다면 $I(X; Y) = 0 \leftrightarrow H(X|Y) =H(X)$
                \end{itemize}
                \item By definition, mutual information has following properties:
                \begin{itemize}
                    \item independence:
                    $$ X \perp Y \  \rightarrow \ I(X;Y) = 0 $$
                    \item symmetry relation: ($\ast$)
                    \begin{align*} I(X ; Y) =H(Y)-H(Y | X) = H(X)-H(X | Y) \end{align*}
                    \vspace{0.5cm}
                \end{itemize}
            \end{itemize}
        \end{frame}

        \begin{frame}{Conditional mutual information}
            \begin{definition}[conditional mutual information]
                The \textbf{conditional mutual information} $I(X;Y|Z)$ is defined as
                \begin{align*}
                    I(X;Y|Z) &\triangleq H(X|Z) - H(X|Y, Z)\\ &= H(Y|Z) - H(Y|X, Z )
                \end{align*}
            \end{definition}
            \begin{itemize}
                \item Conditional mutual information을 정의하기 위해 conditioned r.v. $Z$의 모든 realization에 대한 expectation을 취하여 계산할 수 있다.
                $$I(X;Y|Z) = \mathbb E_{Z}[I(X;Y|Z=z)] $$
                \item 예를 들어, conditioned r.v. $Z$는 channel을 이용한 transmission에서 어떤 channel을 사용할 지 결정하는 요소가 될 수 있다.
            \end{itemize}
        \end{frame}
        
        \begin{frame}{Chain rule}
            \begin{theorem}[chain rule]
                \vspace{-0.4cm}
                %joint r.v.에 대한 설명 (그림추가)
                \begin{align*} I(\underbrace{X_1, X_2}_{joint\  r.v.} ; Y)&=I\left(X_1 ; Y\right)+I\left(X_2 ; Y | X_1\right)\\ &=I(X_2 ; Y) + I(X_1;Y|X_2) \end{align*}
            \end{theorem}
            \vspace{0.3cm}
            $\ast$ \underline{Proof}: 
            \\ $\Rightarrow$
            \vspace{3cm}
        \end{frame}
    \end{section}

    %#################################### 
    \begin{section}{KL-Divergence}
        \begin{frame}{KL-Divergence}
            \begin{definition}[KL-divergence]
                The \textit{relative entropy} or \textbf{Kullback-Leibler distance} between two PMF $P_X(x)$ and $Q_X(x)$ is defined as 
                $$D(P || Q) \triangleq \sum_{x \in \mathcal X} P_X(x) \log \frac{P_X(x)}{Q_X(x)} =\mathbb{E}_{P_X}\left[\log \frac{P(X)}{Q(X)}\right] $$
            \end{definition}
            \begin{itemize}
                \item KL divergence는 동일한 sample space에 대한 서로 다른 확률분포간의 차이; \textit{distance}를 정량화한다.
                \item $||$의 오른쪽에 위치한 PMF $Q$는 분모에 위치하기 때문에, KL divergence가 잘 정의되기 위해서는 다음 조건을 만족해야한다.
                \begin{itemize}
                    \item $P$ is dominated by $Q$ ($P<<Q$) [$\ast$] %그림
                    \item If $Q(X) = 0$, then $P(X)=0$ (for all $x \in \mathcal X$).
                \end{itemize}
            \end{itemize}
        \end{frame}

        \begin{frame}{KL-Divergence}
            By definition, KL-Divergence has following properties:
            \begin{itemize}
                \item KL divergence is \textit{not symmetric}
                $$D(P||Q) \ne D(Q||P)$$
                \item when $P=Q$, then KL divergence is $D(P||Q)=0$
                \item \textbf{information inequality}
            \end{itemize}

            \begin{theorem}[information inequality]
                KL divergence is \textit{non-negative}
                $$ D(P||Q) \ge 0 \text{ with equality iff } P=Q.$$
            \end{theorem}
            $\ast$ \underline{Proof}:
            \vspace{2cm}
        \end{frame}

        \begin{frame}{Chain rule}
            \begin{theorem}[chain rule]
                \vspace{-0.4cm}
                %joint r.v.에 대한 설명 (그림추가)
                \begin{align*} D(P_{X,Y}||Q_{X,Y}) = D(P_X||Q_X) + D(P_{Y|X} || Q_{Y|X})\end{align*}
            \end{theorem}
            \vspace{0.3cm}
            $\ast$ \underline{Proof}: 
            \\ $\Rightarrow$
            \vspace{3cm}
        \end{frame}

    \end{section}

    %#################################### 
    \begin{section}{Remarks about Informations measures}
        \begin{frame}{Another definition of mutual information}
            KL-divergence를 사용하면 mutual information을 다른 관점으로 해석할 수 있다.
            \begin{definition}[mutual informtion]
                Consider two random variables $X$ and $Y$ with a joint PMF $P_{X,Y}(x, y)$ and marginal PMF $P_X(x)$ and $P_Y(y)$.
                The \textbf{mutual information} $I (X; Y )$ is the relative entropy \textit{between the joint distribution} $P_{X,Y}(x,y)$ \textit{and the product distribution} $P_X(x)\cdot P_Y(y)$.
                $$I(X;Y) \triangleq D(P_{X,Y}(x,y) || P_{X}(x)P_Y(y)) = \sum_{x \in \mathcal X} \sum_{y \in \mathcal Y} P_{X,Y}(x,y) \log \frac{P_{X,Y}(x,y)}{P_X(x) P_Y(y)}$$
            \end{definition}
            
            $\ast$ \underline{Proof}: 첫 번째 정의와 두 번째 정의가 동일함을 보이자.
            \\ $\Rightarrow$
            \vspace{3cm}
        
        \end{frame}
        
        \begin{frame}{Useful facts for Information Measures}
            \begin{alertblock}{Fact}
                Here are some easy facts:
                \begin{align*} 
                    H(P_X) &= H(P_U) - D(P_X || P_U) \quad (\ast) \\
                    I(P_X \cdot W) &= D(P_{XY} || (P_X \cdot P_Y))
                \end{align*}
            \end{alertblock}

            $\ast$ \underline{Proof}:
            \\$\Rightarrow$ $D(P_X || P_U)$를 전개하자.
            \vspace{3.5cm}
        \end{frame}

        \begin{frame}{Useful facts for Information Measures}
            \begin{corollary}
                $H(X) \le H(U)$, where $U$ is the uniform distribution over $\mathcal X$ (equality iff $U$ is uniformly distributed)
            \end{corollary}

            \begin{corollary}[onditioning reduces entropy]
                $H(Y) \ge H(Y|X)$, with equality iff $X \bot Y$.
            \end{corollary}
            
            \begin{corollary}
                $I(X; Y) \ge 0$, with equality iff $X \bot Y$.
            \end{corollary}
            
            $\ast$ \underline{Proof}: By the facts, we can easily prove that
            \vspace{3cm}
        
        \end{frame}

        \begin{frame}{Application of Information Measures}
            \begin{itemize}
                \item Entropy: Data compression (e.g, Huffman code)
                \item Mutual information: Data transmission
                \begin{figure}
                    \includegraphics[width=0.55\columnwidth]{image/L2-transmission.png}
                \end{figure}
                \item KL divergence: Hypothesis testing [$\ast$]
                \begin{itemize}
                    \item 어떤 sample space에 대해서 서로 다른 두 hypothesis를 가정해 볼 수 있다.\\ $H_0$: $X \sim P, H_1: X \sim Q$
                    \item $X$를 여러번 관측하여 얻은 realization value를 사용하여얻은 empirical distribution으로부터 실제 probability distribution을 유추할 수 있기 때문에, 어떤 hypothesis가 진실인지 추정할 수 있다.
                    \item Hypothesis testing에서 추정이 틀렸을 확률은 다음과 같이 정의된다.
                    $$P(*) \approx \text{exp}[-n D(P||Q)]$$ 
                    \item 따라서 두 PMF의 KL divergence를 알고있다면 얼마나 많은 데이터($=n$)가 있어야 우리가 원하는 수준의 error-rate를 달성할 수 있는지를 계산할 수 있다.
                    
                \end{itemize}
            \end{itemize}
        \end{frame}
    \end{section}

    %#################################### 
    \begin{section}{Convexity and Concavity of Information Measures}
        \begin{frame}
            \frametitle{Convexity and Concavity}
        
            \begin{definition}[convexity]
                A function $f(x)$ is said to be \textbf{convex} over an interval $(a, b)$ if for every $x_1, x_2 \in (a,b)$ and $0 \le \theta \le 1$,
                $$f((1-\theta) x_1 + \theta x_2) \le (1-\theta)  f(x_1) + \theta f(x_2)$$
                A function $f$ is said to be \textbf{strictly convex} if equality holds only if $\theta = 0$ or $\theta = 1$.
            \end{definition}

            \begin{definition}[concavity]
                A function $f(x)$ is said to be \textbf{concave} over an interval $(a, b)$ if for every $x_1, x_2 \in (a,b)$ and $0 \le \theta \le 1$,
                $$f((1-\theta) x_1 + \theta x_2) \ge (1-\theta)  f(x_1) + \theta f(x_2)$$
                A function $f$ is said to be \textbf{strictly convex} if equality holds only if $\theta = 0$ or $\theta = 1$.

            \end{definition}
            \begin{itemize}
                \item A function $f$ is concave if $-f$ is convex.
                \item convex, concave는 함수가 function value의 linear combination과 비교하여 어떻게 동작하는지를 판단한다.
                \item 어떤 함수가 convex라면, local minimum이 global minimum과 동일하기 때문에 최적화를 쉽게 할 수 있다. 
            \end{itemize}
        \end{frame}

        \begin{frame}{Prerequisites: Log-sum inequality}
            \begin{lemma}[log-sum inequality]
                Given $a_i, b_i \ge 0, i=1,2, \cdots,n,$ let $a=\sum^n_{i=1} a_i$ and $b=\sum^n_{i=1}b_i$. Then we have
                $$ \sum_i a_i \log \frac{a_i}{b_i} \geq a \log \frac{a}{b} , $$
                with equality iff $\dfrac{a_i}{a} = \dfrac{b_i}{b},\ \forall i.$
            \end{lemma}
            
            $\ast$ \underline{Proof}:
            Consider $k_a, k_b > 0$, s.t. $k_a \cdot a = 1,\ k_b \cdot b = 1$ \\ 
            $\Rightarrow$ 
            \vspace{3cm}
        \end{frame}
    
        \begin{frame}
            \frametitle{Convexity of KL-divergence}
            % P_1 P_2가 같은 분포라는 의미는 아님. convexity 따질 떄 f에 들어가는 모든 x에 대해 convex임을 보이기 위해 특별한 임의의 두 점 x_1, x_2를 썼던것처럼 여기서도 모든 PMF PAIR에 대해 convex임을 보여주려고 하는것. 그냥 아랫첨자일뿐 헷갈리지 말자.
            \begin{theorem}
                $D(P||Q)$ is \textbf{convex} in the pair $(P, Q)$;
                That is if $(P_1, Q_1)$ and $(P_2, Q_2)$ are any \alert{two pairs of PMF}, for all $0 \le \theta \le 1$ then 
                $$ D(P_\theta||Q_\theta) \le (1-\theta) D(P_1 || Q_1) + \theta D(P_2 || Q_2)$$
                where $P_\theta = (1-\theta) P_1 + \theta P_2$ and $Q_\theta = (1-\theta) Q_1 + \theta Q_2$.
            \end{theorem}
            \checkmark \underline{meaning}:기존의 PMF를 $\theta$라는 동일한 파라미터에 의해 mixing된 PMF는 각 PMF $P, Q$가 가지고 있던 특징이 줄어들어서 두 분포가 유사하게 변화한다. ($D \Downarrow$)
            \vspace{0.2cm}
            \\$\ast$ \underline{Proof}: By log-sum inequality,
            \\$\Rightarrow$
            \vspace{3cm}
        \end{frame}

        \begin{frame}
            \frametitle{Concavity of Entropy}
            \begin{corollary}
                $H(P)$ is a concave function of $P$
                $$H(P_{\theta}) \ge (1-\theta)  H(P_1) + \theta H(P_2)$$
            \end{corollary}
            \checkmark \underline{meaning}: PMF를 mixing 할수록 점점 uniform distribution에 가깝게 변한다. ($H \Uparrow$)
            \vspace{0.2cm}
            \\ $\ast$ \underline{Proof}: 
            \begin{itemize}
                %다음 함술들은 $P$에 의해서만 결정되는 값들임. (P가 정의역)
                \item (pf.1) $D$가 convex이므로 다음의 관계를 이용하여 쉽게 증명할 수 있다.
                $$H(P) = H(U) - D(P||U)$$
                \item (pf.2) 또는 transmission system에서 source data $X$의 distribution $P_1, P_2$을 결정하는 랜덤 스위치 $Z \sim B(\theta)$를 생각해볼 수 있다.
                \item 따라서 source $X$는 $B(p)$에 따라 mixed distribution $P_\theta$을 따른다.
                \item 이때, $X$의 entropy $H(P_\theta)$는 랜덤 스위치 $Z$의 값을 관측했을 때의 entropy $H(P_\theta|Z)$보다 당연히 크다는 것을 직관적으로 이해할 수 있다.
                \\$\Rightarrow$
                \vspace{1.3cm}
            \end{itemize}
        \end{frame}

        \begin{frame}
            \frametitle{Concavity and Concavity of Mutual Information}
            \begin{corollary}
                The mutual information $I(P_X \cdot W)$ is a \textbf{concave} function of $P_X(x)$ for fixed $W$
                $$ I(P_\theta \cdot W) \ge (1-\theta) I (P_1 \cdot W) + \theta I(P_2 \cdot W)$$
            \end{corollary}
            \checkmark \underline{meaning}: Source data의 distribution을 mixing 할 수록 channel을 통과한 뒤 얻을 수 있는 정보의 양이 더 많아진다.
            \vspace{0.2cm}
            \\ $\ast$ \underline{Proof}: 다음의 system을 가정하자.
            \begin{itemize}
                \item one channel $W$, two source distribution $P_1, P_2$ depended on $Z \sim B(\theta)$.
                $$ X \sim P_{\theta} = (1-\theta)P_1 + \theta P_2$$
                \item (hint: chain rule)
                \\ $\Rightarrow$
            \end{itemize}
            \vspace{4cm}
        \end{frame}

        \begin{frame}
            \frametitle{Concavity and Concavity of Mutual Information}
            \begin{corollary}
                The mutual information $I(P_X \cdot W)$ is a \textbf{convex} function of $W$ for fixed $P_X(x)$
                $$ I(P_X \cdot W_\theta) \le (1-\theta) I (P_X \cdot W_1) + \theta I(P_X\cdot W_2)$$
            \end{corollary}
            \checkmark \underline{meaning}: Channal의 distribution을 mixing 할 수록 channel을 통과한 뒤 얻을 수 있는 정보의 양이 더 줄어든다.
            \vspace{0.2cm}
            \\ $\ast$ \underline{Proof}: 다음의 system을 가정하자.
            \begin{itemize}
                \item one source distribution $P_X$, two channel  $W_1, W_2$ depended on $Z \sim B(\theta)$.
                $$ W_{\theta} = (1-\theta)W_1 + \theta W_2 $$
                \item (hint: chain rule)
                \\ $\Rightarrow$
            \end{itemize}
            \vspace{1.2cm}
            \begin{alertblock}{Conditioning effect on $I$}
                Entropy는 conditioning을 취하면 항상 그 값이 작아지지만 mutual information은 여기서 보인 것처럼 $Z$를 어떻게 설정하는지에 따라 더 커질수도 있고 더 작아질수도 있다.
            \end{alertblock}
        \end{frame}

    \end{section}

    \begin{section}{Data Processing Inequality and Fano's Inequality}
        \begin{frame}
            %그림그려서 설명
            \frametitle{Prerequisites: Markov chain}
            \begin{definition}[Markov chain]
                Random variables$ X, Y, Z$ are said to form a \textbf{Markov chain} in that order (denoted by $X \rightarrow Y \rightarrow Z$) if the conditional distribution of $Z$ depends \textit{only} on $Y$ and is conditionally independent of $X$. ($\ast$)
                $$P_{XYZ}(x, y, z) = P_X(x) P_{Y|X}(y|x) P_{Z|Y}(z|y)$$
                %P_X(x) P_{Y|X}(y|x) P_{Z|YX}(z|yx) = 
            \end{definition}
            \begin{itemize}
                \item Markov chain은 서로 직접적으로 영향받는 변수가 정해져있기 때문에, 아무런 영향을 주지 않는 r.v.에 대한 conditioning은 무시할 수 있다.
                $$P_{Z|YX} = P_{Z|Y}$$
                \item $X \rightarrow Y \rightarrow$ is equivalent to $X \rightarrow Y \rightarrow X$
                \vspace{0.2cm}
                \\ $\ast$ \underline{Proof}: 
                \vspace{1.5cm}
            \end{itemize}
        
        \end{frame}

        \begin{frame}
            \frametitle{Data Processing Inequality}
            \begin{theorem}[data processing inequality]
                If $X \rightarrow Y \rightarrow Z$, then 
                $$ I(X;Y) \ge I(X;Z)$$
                with equality iff $I(X;Y|Z) = 0$, or equivalently 
                \begin{itemize}
                    \item $X \rightarrow Z \rightarrow Y$.
                    \item given $z$, $X \perp Y$
                    \item $I(X;Y) = I(X;Z)$
                \end{itemize}
            \end{theorem}
            \checkmark \underline{meaning}: (data processing) 원본 데이터 $X$에 대해 processing을 수행하면 할 수록 점차 원본과의 관계; mutual information은 줄어든다. ($\leftrightarrow$ 증가할 수 없다.)
            \vspace{0.2cm}
            \\ $\ast$ \underline{Proof}: 
            (By chain rule,)
            \\$\Rightarrow$
            \vspace{2.5cm}
        \end{frame}

        \begin{frame}
            \frametitle{Data Processing Inequality}
            \begin{corollary}[Data Processing Inequality on entropy]
                If $X \rightarrow Y \rightarrow Z$, then 
                $$ H(X|Y) \le H(X|Z)$$
                with equality iff $I(X;Y|Z) = 0$
            \end{corollary}
            \begin{corollary}[Data Processing Inequality on KL-divergence]
                $P_X$ and $Q_X$ is two input distributions and $P_Y$ and $Q_Y$ is two corresponding output distributions, then
                $$D(P_Y || Q_Y) \le D(P_X || Q_X)$$
                where 
                $P_Y(y) = \sum_x P_X(x) W(y|x)$, $Q_Y(y) = \sum_x Q_X(x) W(y|x)$
            \end{corollary}
            \checkmark \underline{meaning}: 서로 다른 분포를 따르는 source였어도, 동일한 channel을 통과하면 data간의 차이가 줄어든다.
            \vspace{0.2cm}
            \\ $\ast$ \underline{Proof}: 
            \vspace{3cm}
        \end{frame}
    
        \begin{frame}
            \frametitle{Fano's Inequality}
            \textbf{System}: 우리가 구하고 싶은 원본 데이터 $X$ 대신, 실제로 관측가능한 데이터 $Y$를 이용하여 $X$의 값을 추정하는 상황을 가정해보자. ($X \rightarrow Y \rightarrow \hat X(Y)$)
            \begin{itemize}
                \item $X$: (hide) data, $Y$: observable data
                \item $\hat X(Y)$: estimate function 
                \item indicator:
                $$\mathbf 1_E := \begin{cases} 0, & \hat X(Y) \ne X \\ 1, & \hat X(Y) = X\end{cases}$$
                \item error probabilistic:
                $$P_e := P( \mathbf{1}_E =0)$$

            \end{itemize}
            \begin{theorem}[Fano's inequality]
                For any estimator $\hat X$, we have error probabilistic's lower bound
                $$H(X|Y) \le \log 2 + P_e \cdot \log(|\mathcal X| - 1)  \ \Leftrightarrow \  \frac{H(X|Y)-\log 2}{\log (|\mathcal X| - 1)} \le P_e$$
            \end{theorem}
            \checkmark \underline{meaning}: $H(X|Y)$의 값이 충분히 작아져야 error probabilistic도 작아질 수 있다. 따라서 $H(X|Y)$의 값을 측정하여 원하는 error rate를 얻기 위해서 얼마나 많은 양의 sample data $Y^n$가 필요할 지 유추할 수 있다.
        \end{frame}

        \begin{frame}
            \frametitle{Fano's Inequality}
            $\ast$ \underline{Proof}: (using chain rule, uniform bound and \textit{data processing inequality})
            $$H(X, \textbf{1}_E|\hat X(Y)) = \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad$$
            \vspace{1cm}
            $$H(X, | \hat X(Y)) = \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad$$
            \vspace{3cm}
        \end{frame}
    \end{section}

    \begin{frame}{Appendix}
        \begin{block}{Notations}
            \begin{itemize}
                \item entropy of r.v. $X \sim P_X$: $H(X), H(P_X)$
                \item conditional entropy of $Y$ conditioned on $X=x$: $H(Y|X=x) , H(P_{Y|X}(\cdot|x))$
                \item conditional entropy of $Y$ conditioned on $X$: $H(Y|X) , H(P_{Y|X})$
                \item mutual information of $X, Y$: $I(X;Y), I(P_X \cdot P_{Y|X}), I(P_X , P_{Y|X})$ \\
                주로 $W= P_{Y|X}$로 두고 $I(P_X \cdot W)$와 같이 표기하여 사용한다.
                \item KL divergence between two distribution $P$ and $Q$: $D(P||Q),  D(P_P||P_Q)$
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}{References}
        \begin{itemize}
            \item T. M. Cover and J. A. Thomas. Elements of Information Theory, Wiley, 2nd ed., 2006.
            \item Lecture notes for EE623: Information Theory (Fall 2024)
        \end{itemize}
        \vspace{6cm}
    \end{frame}

\end{document}